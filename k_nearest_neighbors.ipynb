{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k_nearest_neighbors.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MRC0e0KhQ0S",
        "colab_type": "text"
      },
      "source": [
        "K-Nearest Neighbors or KNNs are one of the most popular classification algorithim's out there. It works on a simple idea of relativity of data. Each new data point is assigned a category based on the highest number of \"neighbors\" that are close by with Euclidean distance. This dataset is another widely used dataset from Kaggle that represents certain metrics for users who saw an e-commerce/social media ad for a product and whether they bought it or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWd1UlMnhT2s",
        "colab_type": "text"
      },
      "source": [
        "### Importing the libraries\n",
        "These are the three go to libraries for most ML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvGPUQaHhXfL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1VMqkGvhc3-",
        "colab_type": "text"
      },
      "source": [
        "### Importing the dataset\n",
        "I imported the dataset through Pandas dataframe then using iloc assign everything besides the last column as our independent variable(s) or X and the last column as our dependent variable or Y. The name of the dataset has to be updated and it must be in the same folder as your .py file or uploaded on Jupyter Notebooks or Google Collab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M52QDmyzhh9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = pd.read_csv('Social_Network_Ads.csv')\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvxIPVyMhmKp",
        "colab_type": "text"
      },
      "source": [
        "### Splitting the dataset into the Training set and Test set\n",
        "Here I used a normal 80/20 test size and also assign 'random_state' to 0 for consistency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVzJWAXIhxoC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8qBMqonQV1PJ"
      },
      "source": [
        "### Feature Scaling\n",
        "We can feature scale our data to make it easier for our model to train on our data and give us accurate results that aren't shifted by the presence of extreme outliers. It's not necessary, but helpful for getting more accurate results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uKJtcJNuV1PL",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb6jCOCQiAmP",
        "colab_type": "text"
      },
      "source": [
        "### Training the K-NN model on the Training set\n",
        "I chose 5 as the 'n_neighbors' as its the most common and chose 2 as the 'p' (not p-value, powet parameter) for the minkowski metric which is the deafult and useful distance for vector spaces."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0pFVAmciHQs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "classifier = KNeighborsClassifier(n_neighbors = 5, p = 2)\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyxW5b395mR2",
        "colab_type": "text"
      },
      "source": [
        "### Predicting a new result\n",
        "This is used to make a prediction on whether a particular person would the product."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8YOXsQy58rP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(classifier.predict(sc.transform([[age,income]])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4Hwj34ziWQW",
        "colab_type": "text"
      },
      "source": [
        "### Confusion Matrix\n",
        "The confusion matrix is a useful metric for classification models to allow us to visualize the correct positive, false positive, false negative, and correct negative predictions as well as a ultimate accuracy score on the bottom. Here we acheive a whopping 95% accuracy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6bpZwUiiXic",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "y_pred = classifier.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(cm)\n",
        "accuracy_score(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OMC_P0diaoD",
        "colab_type": "text"
      },
      "source": [
        "### Visualising the Training set results\n",
        "It's useful to first visualize our training set results so we can see how our model made wrong/correct predictions and in what way the data is split. Here we use a meshgrid to arrange our predictions in almost a scatter plot formation and use the enumerate function with a contour line to show where our model made a positive prediction (green area) and where it made a negative prediction(red area)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NOjKvZRid5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = sc.inverse_transform(X_train), y_train\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1), np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\n",
        "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('K-NN (Training set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ-j28aPihZx",
        "colab_type": "text"
      },
      "source": [
        "## Visualising the Test set results\n",
        "Same thing as above but visualizing our test results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeTjz2vDilAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "X_set, y_set = sc.inverse_transform(X_test), y_test\n",
        "X1, X2 = np.meshgrid(np.arange(start = X_set[:, 0].min() - 10, stop = X_set[:, 0].max() + 10, step = 1), np.arange(start = X_set[:, 1].min() - 1000, stop = X_set[:, 1].max() + 1000, step = 1))\n",
        "plt.contourf(X1, X2, classifier.predict(sc.transform(np.array([X1.ravel(), X2.ravel()]).T)).reshape(X1.shape), alpha = 0.75, cmap = ListedColormap(('red', 'green')))\n",
        "plt.xlim(X1.min(), X1.max())\n",
        "plt.ylim(X2.min(), X2.max())\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1], c = ListedColormap(('red', 'green'))(i), label = j)\n",
        "plt.title('K-NN (Test set)')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Estimated Salary')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}